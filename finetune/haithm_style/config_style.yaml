# Haithm Style Finetuning Configuration
# V1 â€“ text-only, light run (no audio, no prompts dataset)

base_model_name: "Qwen/Qwen2.5-3B-Instruct"

dataset_path_natural: "data/dataset_haithm_style_natural.jsonl"
dataset_path_prompts: "data/dataset_haithm_style_prompts.jsonl"
use_prompts_dataset: false

output_dir_base: "models/hva_haithm_style_lora"

hyperparameters:
  num_train_epochs: 1
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 1
  learning_rate: 2.0e-4
  max_grad_norm: 0.3
  warmup_ratio: 0.03
  max_seq_length: 1024
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  max_steps: 30
