{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Haithm Style Fine-Tuning (Qwen 3B QLoRA)\n",
                "\n",
                "This notebook fine-tunes the Qwen 2.5 3B Instruct model on Haithm's personal writing style using QLoRA.\n",
                "\n",
                "## 1. Setup\n",
                "**Before running:**\n",
                "1. Upload your dataset files (`dataset_haithm_style_natural.jsonl`, `dataset_haithm_style_prompts.jsonl`) to the Colab runtime (drag & drop to file sidebar).\n",
                "2. Enable GPU Runtime: `Runtime` -> `Change runtime type` -> `T4 GPU` (or better)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q -U torch transformers peft datasets bitsandbytes trl"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Model & Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from datasets import load_dataset, concatenate_datasets\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
                "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
                "from trl import SFTTrainer\n",
                "\n",
                "# Config\n",
                "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
                "dataset_natural = \"dataset_haithm_style_natural.jsonl\"\n",
                "dataset_prompts = \"dataset_haithm_style_prompts.jsonl\"\n",
                "new_model = \"qwen-3b-haithm-style-lora\"\n",
                "\n",
                "# Load Datasets\n",
                "print(\"Loading datasets...\")\n",
                "dataset = load_dataset(\"json\", data_files={\"train\": dataset_natural}, split=\"train\")\n",
                "\n",
                "try:\n",
                "    prompts_ds = load_dataset(\"json\", data_files={\"train\": dataset_prompts}, split=\"train\")\n",
                "    # Oversample prompts (x5) to ensure they have impact\n",
                "    for _ in range(5):\n",
                "        dataset = concatenate_datasets([dataset, prompts_ds])\n",
                "    print(f\"Merged datasets. Total examples: {len(dataset)}\")\n",
                "except Exception as e:\n",
                "    print(f\"Warning: Could not load prompts dataset: {e}\")\n",
                "\n",
                "# 4-bit Quantization\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                "    bnb_4bit_use_double_quant=False,\n",
                ")\n",
                "\n",
                "# Load Base Model\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_name,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True\n",
                ")\n",
                "model.config.use_cache = False\n",
                "model.config.pretraining_tp = 1\n",
                "\n",
                "# Load Tokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.padding_side = \"right\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training (QLoRA)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# LoRA Config\n",
                "peft_config = LoraConfig(\n",
                "    lora_alpha=16,\n",
                "    lora_dropout=0.1,\n",
                "    r=64,\n",
                "    bias=\"none\",\n",
                "    task_type=\"CAUSAL_LM\",\n",
                "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
                ")\n",
                "\n",
                "# Training Params\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./results\",\n",
                "    num_train_epochs=1,\n",
                "    per_device_train_batch_size=4,\n",
                "    gradient_accumulation_steps=1,\n",
                "    optim=\"paged_adamw_32bit\",\n",
                "    save_steps=25,\n",
                "    logging_steps=25,\n",
                "    learning_rate=2e-4,\n",
                "    weight_decay=0.001,\n",
                "    fp16=True,\n",
                "    bf16=False,\n",
                "    max_grad_norm=0.3,\n",
                "    max_steps=-1,\n",
                "    warmup_ratio=0.03,\n",
                "    group_by_length=True,\n",
                "    lr_scheduler_type=\"constant\",\n",
                ")\n",
                "\n",
                "# Traverse\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    train_dataset=dataset,\n",
                "    peft_config=peft_config,\n",
                "    dataset_text_field=\"output\",\n",
                "    max_seq_length=2048,\n",
                "    tokenizer=tokenizer,\n",
                "    args=training_args,\n",
                "    packing=False,\n",
                ")\n",
                "\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Save Adapter"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import locale\n",
                "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
                "\n",
                "trainer.model.save_pretrained(new_model)\n",
                "tokenizer.save_pretrained(new_model)\n",
                "\n",
                "print(f\"Model saved to {new_model}\")\n",
                "\n",
                "# Zip for download\n",
                "!zip -r {new_model}.zip {new_model}\n",
                "print(\"Download the zip file from the file browser.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}