{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Haithm Style Fine-Tuning (Qwen 3B QLoRA)\n",
                "\n",
                "This notebook fine-tunes the Qwen 2.5 3B Instruct model on Haithm's personal writing style using QLoRA.\n",
                "\n",
                "## 1. Setup\n",
                "**Before running:**\n",
                "1. Upload your dataset files (`dataset_haithm_style_natural.jsonl`, `dataset_haithm_style_prompts.jsonl`) to the Colab runtime (drag & drop to file sidebar).\n",
                "2. Enable GPU Runtime: `Runtime` -> `Change runtime type` -> `T4 GPU` (or better)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q -U torch transformers peft datasets bitsandbytes trl"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Model & Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# [ANTIGRAVITY] EXPORT USED DATASET (AUDIT CELL)\n",
                "# Run this IMMEDIATELY after defining 'trainer' and BEFORE 'trainer.train()'\n",
                "\n",
                "import os\n",
                "import json\n",
                "import hashlib\n",
                "\n",
                "RUN_ID = \"hs-20251213-v2.5\"\n",
                "AUDIT_DIR = f\"/content/run_audit/{RUN_ID}\"\n",
                "os.makedirs(AUDIT_DIR, exist_ok=True)\n",
                "\n",
                "def get_content_hash(text):\n",
                "    return hashlib.sha256(str(text).encode(\"utf-8\")).hexdigest()\n",
                "\n",
                "def get_file_hash(filepath):\n",
                "    if not os.path.exists(filepath): return \"MISSING\", 0, 0\n",
                "    with open(filepath, \"rb\") as f:\n",
                "        data = f.read()\n",
                "    return hashlib.sha256(data).hexdigest(), len(data), len(data.splitlines())\n",
                "\n",
                "def scan_artifacts(text):\n",
                "    artifacts = [\"\\ue200filecite\", \"turn0file\", \"turn1file\", \"[STATE:\"]\n",
                "    found = []\n",
                "    for art in artifacts:\n",
                "        if art in text:\n",
                "            found.append(art)\n",
                "    return found\n",
                "\n",
                "def export_hf_dataset(hf_ds, filename):\n",
                "    if not hf_ds:\n",
                "        print(f\"Skipping {filename} (None)\")\n",
                "        return None\n",
                "        \n",
                "    out_path = os.path.join(AUDIT_DIR, filename)\n",
                "    count = 0\n",
                "    artifact_count = 0\n",
                "    \n",
                "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
                "        for i, item in enumerate(hf_ds):\n",
                "            # Extract text content for canonical ID\n",
                "            # SFTTrainer default field is often 'text' after formatting, or 'input_ids' if tokenized.\n",
                "            # If packing=False, it preserves original columns too.\n",
                "            content_str = str(item)\n",
                "            if \"text\" in item: content_str = item[\"text\"]\n",
                "            elif \"output\" in item: content_str = item[\"output\"] \n",
                "            \n",
                "            # Generate Stable ID\n",
                "            if \"example_id\" not in item:\n",
                "                item[\"example_id\"] = get_content_hash(content_str)[:16]\n",
                "            \n",
                "            # Artifact Scan\n",
                "            found_arts = scan_artifacts(content_str)\n",
                "            if found_arts:\n",
                "                artifact_count += 1\n",
                "                if count < 5: # Log first few\n",
                "                    print(f\"[Audit Warning] Artifacts {found_arts} found in {filename} row {i}\")\n",
                "\n",
                "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
                "            count += 1\n",
                "    \n",
                "    # Compute Hash of Exported File\n",
                "    file_hash, bytes_count, lines = get_file_hash(out_path)\n",
                "    print(f\"Exported {filename}: {count} rows, Hash: {file_hash[:8]}..., Artifacts found: {artifact_count}\")\n",
                "    return {\"filename\": filename, \"count\": count, \"sha256\": file_hash, \"bytes\": bytes_count, \"artifacts_found\": artifact_count}\n",
                "\n",
                "print(\"--- Starting Audit Export ---\")\n",
                "\n",
                "# 1. Source Manifest\n",
                "source_files = [\n",
                "    \"dataset_haithm_style_natural_v2.jsonl\",\n",
                "    \"dataset_haithm_style_prompts.jsonl\",\n",
                "    \"dataset_haithm_style_persona_v2.jsonl\",\n",
                "    \"dataset_haithm_style_cognitive_v2.jsonl\",\n",
                "    \"dataset_haithm_v3_cognitive_map.jsonl\"\n",
                "]\n",
                "source_manifest = []\n",
                "for fname in source_files:\n",
                "    h, b, l = get_file_hash(fname)\n",
                "    source_manifest.append({\"filename\": fname, \"sha256\": h, \"bytes\": b, \"lines\": l})\n",
                "\n",
                "# 2. Export Trainer Datasets\n",
                "train_meta = export_hf_dataset(trainer.train_dataset, \"train_used.jsonl\")\n",
                "eval_meta = export_hf_dataset(trainer.eval_dataset, \"val_used.jsonl\")\n",
                "\n",
                "# 3. Final Manifest\n",
                "manifest_used = {\n",
                "    \"run_id\": RUN_ID,\n",
                "    \"source_files\": source_manifest,\n",
                "    \"exported_datasets\": [d for d in [train_meta, eval_meta] if d]\n",
                "}\n",
                "\n",
                "with open(os.path.join(AUDIT_DIR, \"manifest_used.json\"), \"w\") as f:\n",
                "    json.dump(manifest_used, f, indent=4)\n",
                "\n",
                "print(f\"Audit Complete. Manifest saved to {AUDIT_DIR}/manifest_used.json\")\n",
                "print(json.dumps(manifest_used, indent=2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from datasets import load_dataset, concatenate_datasets\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
                "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
                "from trl import SFTTrainer\n",
                "\n",
                "# Config\n",
                "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
                "new_model = \"qwen-3b-haithm-style-lora\"\n",
                "\n",
                "# Load Datasets (Weighted Mixing)\n",
                "print(\"Loading datasets with V2.5 weights...\")\n",
                "\n",
                "# 1. Load raw files\n",
                "ds_natural = load_dataset(\"json\", data_files={\"train\": \"dataset_haithm_style_natural_v2.jsonl\"}, split=\"train\")\n",
                "ds_prompts = load_dataset(\"json\", data_files={\"train\": \"dataset_haithm_style_prompts.jsonl\"}, split=\"train\")\n",
                "ds_persona = load_dataset(\"json\", data_files={\"train\": \"dataset_haithm_style_persona_v2.jsonl\"}, split=\"train\")\n",
                "ds_cognitive = load_dataset(\"json\", data_files={\"train\": \"dataset_haithm_style_cognitive_v2.jsonl\"}, split=\"train\")\n",
                "ds_map = load_dataset(\"json\", data_files={\"train\": \"dataset_haithm_v3_cognitive_map.jsonl\"}, split=\"train\")\n",
                "\n",
                "# 2. Apply Weights (Concatenation)\n",
                "# Weights: Natural=1, Prompts=3, Persona=6, Cognitive=6, Map=50\n",
                "datasets_to_merge = []\n",
                "\n",
                "datasets_to_merge.append(ds_natural) # x1\n",
                "\n",
                "for _ in range(3):\n",
                "    datasets_to_merge.append(ds_prompts)\n",
                "\n",
                "for _ in range(6):\n",
                "    datasets_to_merge.append(ds_persona)\n",
                "\n",
                "for _ in range(6):\n",
                "    datasets_to_merge.append(ds_cognitive)\n",
                "    \n",
                "for _ in range(50):\n",
                "    datasets_to_merge.append(ds_map)\n",
                "\n",
                "dataset = concatenate_datasets(datasets_to_merge)\n",
                "dataset = dataset.shuffle(seed=42) # Shuffle to mix them\n",
                "\n",
                "print(f\"Merged datasets. Total examples: {len(dataset)}\")\n",
                "\n",
                "# 4-bit Quantization\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                "    bnb_4bit_use_double_quant=False,\n",
                ")\n",
                "\n",
                "# Load Base Model\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_name,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True\n",
                ")\n",
                "model.config.use_cache = False\n",
                "model.config.pretraining_tp = 1\n",
                "\n",
                "# Load Tokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.padding_side = \"right\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training (QLoRA)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# LoRA Config\n",
                "peft_config = LoraConfig(\n",
                "    lora_alpha=16,\n",
                "    lora_dropout=0.1,\n",
                "    r=64,\n",
                "    bias=\"none\",\n",
                "    task_type=\"CAUSAL_LM\",\n",
                "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
                ")\n",
                "\n",
                "# Training Params\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./results\",\n",
                "    num_train_epochs=1,\n",
                "    per_device_train_batch_size=4,\n",
                "    gradient_accumulation_steps=1,\n",
                "    optim=\"paged_adamw_32bit\",\n",
                "    save_steps=25,\n",
                "    logging_steps=25,\n",
                "    learning_rate=2e-4,\n",
                "    weight_decay=0.001,\n",
                "    fp16=True,\n",
                "    bf16=False,\n",
                "    max_grad_norm=0.3,\n",
                "    max_steps=-1,\n",
                "    warmup_ratio=0.03,\n",
                "    group_by_length=True,\n",
                "    lr_scheduler_type=\"constant\",\n",
                ")\n",
                "\n",
                "# Traverse\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    train_dataset=dataset,\n",
                "    peft_config=peft_config,\n",
                "    dataset_text_field=\"output\",\n",
                "    max_seq_length=2048,\n",
                "    tokenizer=tokenizer,\n",
                "    args=training_args,\n",
                "    packing=False,\n",
                ")\n",
                "\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Save Adapter"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import locale\n",
                "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
                "\n",
                "trainer.model.save_pretrained(new_model)\n",
                "tokenizer.save_pretrained(new_model)\n",
                "\n",
                "print(f\"Model saved to {new_model}\")\n",
                "\n",
                "# Zip for download\n",
                "!zip -r {new_model}.zip {new_model}\n",
                "print(\"Download the zip file from the file browser.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}