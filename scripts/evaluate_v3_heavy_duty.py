
import json
import time
import torch
import os
import re
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel 

# --- CONFIGURATION ---
BASE_MODEL_NAME = "Qwen/Qwen2.5-3B-Instruct"
ADAPTER_PATH = "models/hva_haithm_style_lora_v2"
REPORT_FILE = "docs/V3_Heavy_Duty_Report.md"

# --- DATASETS ---

# 1. Identity & Persona (Trap Questions)
# Testing for: Repetition, "As an AI", "ChatGPT", Formal MSA vs Dialect
PERSONA_TESTS = [
    {"q": "Ù…ÙŠÙ† Ø£Ù†ØªØŸ", "type": "Identity"},
    {"q": "Ø£Ù†Øª Ù…Ù† Ø´Ø±ÙƒØ© OpenAIØŸ", "type": "Company_Trap"},
    {"q": "ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ Ø§Ù„ÙŠÙˆÙ… ÙŠØ§ ÙˆØ­Ø´ØŸ", "type": "Dialect_Chat"},
    {"q": "Ø§ÙƒØªØ¨ Ù„ÙŠ Ù‚ØµÙŠØ¯Ø© Ø¹Ù† Ø§Ù„ØµØ¨Ø±.", "type": "Refusal_Check (Should happen or be brief)"},
    {"q": "Ù„ÙŠØ´ ØµÙˆØªÙƒ Ù…ØªØºÙŠØ±ØŸ", "type": "Context_Awareness"},
    {"q": "Ù…ÙŠÙ† Ù…Ø¹Ù„Ù…ÙƒØŸ", "type": "Creator_Credit"},
    {"q": "ÙˆØ´ Ø±Ø£ÙŠÙƒ ÙÙŠ Ø§Ù„Ù„ÙŠ ÙŠØµÙŠØ± Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…ØŸ", "type": "Opinion_Trap"},
    {"q": "Ø¹Ø±Ù Ù†ÙØ³Ùƒ Ø¨Ù„Ù‡Ø¬Ø© Ø³Ø¹ÙˆØ¯ÙŠØ© Ù‚Ø­Ø©.", "type": "Dialect_Force"},
]

# 2. Complex JSON (Param Extraction & Edge Cases)
JSON_TESTS = [
    # Basic
    {"cmd": "Ø§ÙØªØ­ Ø§Ù„ØªÙŠØ±Ù…ÙŠÙ†Ø§Ù„", "expect": "open_app"},
    {"cmd": "Ø·ÙÙŠ Ø§Ù„Ù†ÙˆØ±", "expect": "smart_home"},
    
    # Complex / Nested
    {"cmd": "Ø§Ø¨Ø­Ø« Ø¹Ù† ÙƒÙ„ Ù…Ù„ÙØ§Øª Ø§Ù„Ù€ PDF ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯ 'Ù…Ø´Ø§Ø±ÙŠØ¹' Ø§Ù„Ù„ÙŠ Ø¹Ø¯Ù„ØªÙ‡Ø§ Ø§Ù…Ø³", "expect": "search_files", "complexity": "High"},
    {"cmd": "Ø§Ø±Ø³Ù„ Ø§ÙŠÙ…ÙŠÙ„ Ù„Ø£Ø­Ù…Ø¯ Ù‚Ù„Ù‡ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ ØªØ£Ø¬Ù„", "expect": "send_email", "complexity": "High"},
    {"cmd": "Ø°ÙƒØ±Ù†ÙŠ Ø¨Ø¹Ø¯ Ù¥ Ø¯Ù‚Ø§ÙŠÙ‚ Ø§Ø·ÙÙŠ Ø§Ù„ÙØ±Ù†", "expect": "set_reminder", "complexity": "High"},
    
    # Dialect/Ambiguity Traps
    {"cmd": "Ø³ÙƒØ± Ø§Ù„Ø¬Ù‡Ø§Ø²", "expect": "system_control", "note": "sugar vs close"},
    {"cmd": "Ø³ÙƒØ± Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹", "expect": "ignore/cancel", "note": "metaphorical close"},
    {"cmd": "Ø´ÙˆÙ Ù„ÙŠ Ø­Ù„ ÙÙŠ Ø§Ù„Ù†Øª", "expect": "web_search", "note": "vague intent"},
    
    # Negative / Impossible
    {"cmd": "Ø³ÙˆÙŠ Ù„ÙŠ ÙƒØ¨Ø³Ø©", "expect": "unknown/refusal", "note": "physical action"},
    {"cmd": "Ø§Ø­Ùƒ Ù„ÙŠ Ù†ÙƒØªØ©", "expect": "chat_mode", "note": "not a command"}
]

SYSTEM_PROMPT_JSON = """System: You are 'Haitham Agent'. 
Input: User voice command.
Output: VALID JSON ONLY. No markdown, no explanations.
Schema: {"tool": "tool_name", "params": {"key": "value"}}"""

def load_model(device):
    print(f"ğŸ”¹ Loading Base Model: {BASE_MODEL_NAME} on {device}...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_NAME,
        torch_dtype=torch.float16 if device != "cpu" else torch.float32,
        device_map=device,
        trust_remote_code=True
    )
    print(f"ğŸ”¸ Loading Adapter: {ADAPTER_PATH}")
    model = PeftModel.from_pretrained(model, ADAPTER_PATH)
    return model, tokenizer

def check_repetition(prompt, response):
    # Check if a significant part of the prompt is repeated at the start of response
    # Normalize: remove punctuation, lowercase
    p_norm = re.sub(r'[^\w\s]', '', prompt).strip()
    r_norm = re.sub(r'[^\w\s]', '', response).strip()
    
    # If the first 50% of prompt chars appear in first 50% of response
    if p_norm in r_norm[:len(p_norm)+20]:
        return True
    return False

def run_persona_test(model, tokenizer, device):
    print("\nğŸ§  RUNNING HEAVY DUTY PERSONA CHECK...")
    results = []
    
    for item in PERSONA_TESTS:
        q = item['q']
        print(f"   [Type: {item['type']}] Asking: '{q}' ... ", end="", flush=True)
        
        inputs = tokenizer(q, return_tensors="pt").to(device)
        start = time.time()
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=200,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        duration = time.time() - start
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # ANALYSIS
        repetition = check_repetition(q, response)
        chatgpt_mention = "chatgpt" in response.lower() or "openai" in response.lower()
        assistant_mention = "Ù…Ø³Ø§Ø¹Ø¯" in response or "assistant" in response.lower()
        
        status = "âœ… PASS"
        issues = []
        if repetition: 
            status = "âš ï¸ REPEAT"
            issues.append("Repetition")
        if chatgpt_mention:
            status = "âŒ FAIL"
            issues.append("Identity_Hallucination")
        if assistant_mention:
            status = "âš ï¸ WEAK"
            issues.append("Generic_Persona")
            
        results.append({
            "q": q,
            "response": response,
            "status": status,
            "issues": issues,
            "time": duration
        })
        print(f"{status} ({duration:.2f}s)")
        
    return results

def run_json_test(model, tokenizer, device):
    print("\nâš¡ RUNNING HEAVY DUTY JSON CHECK...")
    results = []
    
    for item in JSON_TESTS:
        cmd = item['cmd']
        print(f"   Cmd: '{cmd}' ... ", end="", flush=True)
        
        # Strict Prompting
        full_prompt = f"{SYSTEM_PROMPT_JSON}\nUser: {cmd}\nOutput:"
        inputs = tokenizer(full_prompt, return_tensors="pt").to(device)
        
        start = time.time()
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=150,
                temperature=0.1,
                do_sample=False,
                pad_token_id=tokenizer.eos_token_id
            )
        duration = time.time() - start
        raw = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extract JSON
        json_str = ""
        if "Output:" in raw:
            json_str = raw.split("Output:")[-1].strip()
        else:
            # Fallback extraction
            match = re.search(r'\{.*\}', raw, re.DOTALL)
            if match:
                json_str = match.group(0)
            else:
                json_str = raw.strip()

        # Validation
        valid_json = False
        parsed = {}
        try:
            parsed = json.loads(json_str)
            valid_json = True
            # Check structure roughly
            if "tool" in parsed or "action" in parsed:
                pass # Good
        except:
            valid_json = False
            
        status = "âœ… VALID" if valid_json else "âŒ INVALID"
        
        # Check logic (did it get the right tool?)
        logic_check = "â“"
        if valid_json:
            val_str = str(parsed).lower()
            expected = item.get("expect", "").lower()
            if expected in val_str:
                logic_check = "âœ… LOGIC OK"
            else:
                logic_check = f"âš ï¸ LOGIC DIFF ({expected})"
                
        results.append({
            "cmd": cmd,
            "raw": json_str,
            "valid": valid_json,
            "logic": logic_check,
            "time": duration
        })
        print(f"{status} | {logic_check}")
        
    return results

def generate_full_report(p_res, j_res):
    # Stats
    p_fail = len([r for r in p_res if "FAIL" in r['status']])
    p_warn = len([r for r in p_res if "WARN" in r['status'] or "REPEAT" in r['status']])
    j_valid = len([r for r in j_res if r['valid']])
    j_total = len(j_res)
    
    report = f"""# ğŸ›¡ï¸ V3 Heavy Duty Evaluation Report
**Model:** Qwen 2.5 3B + Adapter V2
**Date:** {time.strftime('%Y-%m-%d %H:%M')}

## 1. Executive Summary
- **JSON Reliability:** {j_valid}/{j_total} ({(j_valid/j_total)*100:.1f}%)
- **Persona Integrity:** {len(p_res) - p_fail - p_warn}/{len(p_res)} Clean Responses
- **Critical Failures:** {p_fail} (Identity Hallucinations)

## 2. Persona Deep Dive
| Question | Status | Issues | Response Preview |
|----------|--------|--------|------------------|
"""
    for r in p_res:
        preview = r['response'].replace("\n", " ")[:100]
        report += f"| {r['q']} | {r['status']} | {', '.join(r['issues'])} | {preview} |\n"
        
    report += """
## 3. JSON Stress Test
| Command | Valid? | Logic Check | Output |
|---------|--------|-------------|--------|
"""
    for r in j_res:
        valid_mark = "âœ…" if r['valid'] else "âŒ"
        out_prev = r['raw'].replace("\n", "")[:50]
        report += f"| {r['cmd']} | {valid_mark} | {r['logic']} | `{out_prev}` |\n"
        
    with open(REPORT_FILE, "w") as f:
        f.write(report)
    print(f"\nğŸ“„ Full Report: {REPORT_FILE}")

def main():
    device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
    try:
        model, tokenizer = load_model(device)
        p_res = run_persona_test(model, tokenizer, device)
        j_res = run_json_test(model, tokenizer, device)
        generate_full_report(p_res, j_res)
    except Exception as e:
        print(f"FATAL ERROR: {e}")

if __name__ == "__main__":
    main()
